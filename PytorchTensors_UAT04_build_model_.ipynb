{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO/PoMUNzflzYwznt7i8o4T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nallagondu/pytorch-tutorial/blob/master/PytorchTensors_UAT04_build_model_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building Models with PyTorch**\n",
        "\n",
        "torch.nn.Module and torch.nn.Parameter\n"
      ],
      "metadata": {
        "id": "C4vVai8Zm3dH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOTSKMWamv1x"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class TinyModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(TinyModel, self).__init__()\n",
        "\n",
        "        self.linear1 = torch.nn.Linear(100, 200)\n",
        "        self.activation = torch.nn.ReLU()\n",
        "        self.linear2 = torch.nn.Linear(200, 10)\n",
        "        self.softmax = torch.nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "tinymodel = TinyModel()\n",
        "\n",
        "print('The model:')\n",
        "print(tinymodel)\n",
        "\n",
        "print('\\n\\nJust one layer:')\n",
        "print(tinymodel.linear2)\n",
        "\n",
        "print('\\n\\nModel params:')\n",
        "for param in tinymodel.parameters():\n",
        "    print(param)\n",
        "\n",
        "print('\\n\\nLayer params:')\n",
        "for param in tinymodel.linear2.parameters():\n",
        "    print(param)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lin = torch.nn.Linear(3, 2)\n",
        "x = torch.rand(1, 3)\n",
        "print('Input:')\n",
        "print(x)\n",
        "\n",
        "print('\\n\\nWeight and Bias parameters:')\n",
        "for param in lin.parameters():\n",
        "    print(param)\n",
        "\n",
        "y = lin(x)\n",
        "print('\\n\\nOutput:')\n",
        "print(y)"
      ],
      "metadata": {
        "id": "xvm8F-_jn55Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.functional as F\n",
        "\n",
        "\n",
        "class LeNet(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
        "        self.fc2 = torch.nn.Linear(120, 84)\n",
        "        self.fc3 = torch.nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square you can only specify a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features"
      ],
      "metadata": {
        "id": "O_1UZwIboQgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####PyTorch TensorBoard Support -->"
      ],
      "metadata": {
        "id": "yWEo8VAeootf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision matplotlib tensorboard"
      ],
      "metadata": {
        "id": "5_QBKJuPogVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch model and training necessities\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import tensorflow as tf\n",
        "import tensorboard as tb\n",
        "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
        "\n",
        "# Image datasets and image manipulation\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Image display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# PyTorch TensorBoard support\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# In case you are using an environment that has TensorFlow installed,\n",
        "# such as Google Colab, uncomment the following code to avoid\n",
        "# a bug with saving embeddings to your TensorBoard directory\n",
        "\n",
        "# import tensorflow as tf\n",
        "# import tensorboard as tb\n",
        "# tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
      ],
      "metadata": {
        "id": "mpAZok26ow4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gather datasets and prepare them for consumption\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Store separate training and validations splits in ./data\n",
        "training_set = torchvision.datasets.FashionMNIST('./data',\n",
        "    download=True,\n",
        "    train=True,\n",
        "    transform=transform)\n",
        "validation_set = torchvision.datasets.FashionMNIST('./data',\n",
        "    download=True,\n",
        "    train=False,\n",
        "    transform=transform)\n",
        "\n",
        "training_loader = torch.utils.data.DataLoader(training_set,\n",
        "                                              batch_size=4,\n",
        "                                              shuffle=True,\n",
        "                                              num_workers=2)\n",
        "\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(validation_set,\n",
        "                                                batch_size=4,\n",
        "                                                shuffle=False,\n",
        "                                                num_workers=2)\n",
        "\n",
        "# Class labels\n",
        "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
        "\n",
        "# Helper function for inline image display\n",
        "def matplotlib_imshow(img, one_channel=False):\n",
        "    if one_channel:\n",
        "        img = img.mean(dim=0)\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    if one_channel:\n",
        "        plt.imshow(npimg, cmap=\"Greys\")\n",
        "    else:\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "# Extract a batch of 4 images\n",
        "dataiter = iter(training_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Create a grid from the images and show them\n",
        "img_grid = torchvision.utils.make_grid(images)\n",
        "matplotlib_imshow(img_grid, one_channel=True)"
      ],
      "metadata": {
        "id": "vZ5eJcc6pOJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Default log_dir argument is \"runs\" - but it's good to be specific\n",
        "# torch.utils.tensorboard.SummaryWriter is imported above\n",
        "writer = SummaryWriter('runs/fashion_mnist_experiment_1')\n",
        "\n",
        "# Write image data to TensorBoard log dir\n",
        "writer.add_image('Four Fashion-MNIST Images', img_grid)\n",
        "writer.flush()\n",
        "\n",
        "# To view, start TensorBoard on the command line with:\n",
        "#   tensorboard --logdir=runs\n",
        "# ...and open a browser tab to http://localhost:6006/"
      ],
      "metadata": {
        "id": "9X6WCi4drY2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graphing Scalars to Visualize Training\n",
        "TensorBoard is useful for tracking the progress and efficacy of your training. Below, we’ll run a training loop, track some metrics, and save the data for TensorBoard’s consumption.\n",
        "\n",
        "Let’s define a model to categorize our image tiles, and an optimizer and loss function for training:"
      ],
      "metadata": {
        "id": "GrDFVUjcrhpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "hPJSf8dNrgQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(validation_loader))\n",
        "for epoch in range(1):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(training_loader, 0):\n",
        "        # basic training loop\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 999:    # Every 1000 mini-batches...\n",
        "            print('Batch {}'.format(i + 1))\n",
        "            # Check against the validation set\n",
        "            running_vloss = 0.0\n",
        "\n",
        "            # In evaluation mode some model specific operations can be omitted eg. dropout layer\n",
        "            net.train(False) # Switching to evaluation mode, eg. turning off regularisation\n",
        "            for j, vdata in enumerate(validation_loader, 0):\n",
        "                vinputs, vlabels = vdata\n",
        "                voutputs = net(vinputs)\n",
        "                vloss = criterion(voutputs, vlabels)\n",
        "                running_vloss += vloss.item()\n",
        "            net.train(True) # Switching back to training mode, eg. turning on regularisation\n",
        "\n",
        "            avg_loss = running_loss / 1000\n",
        "            avg_vloss = running_vloss / len(validation_loader)\n",
        "\n",
        "            # Log the running loss averaged per batch\n",
        "            writer.add_scalars('Training vs. Validation Loss',\n",
        "                            { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                            epoch * len(training_loader) + i)\n",
        "\n",
        "            running_loss = 0.0\n",
        "print('Finished Training')\n",
        "\n",
        "writer.flush()"
      ],
      "metadata": {
        "id": "LHQ4fVgzrrPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing Your Model**\n",
        "\n",
        "TensorBoard can also be used to examine the data flow within your model. To do this, call the add_graph() method with a model and sample input:\n",
        "\n"
      ],
      "metadata": {
        "id": "r2QMAH4-r0mZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Again, grab a single mini-batch of images\n",
        "dataiter = iter(training_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# add_graph() will trace the sample input through your model,\n",
        "# and render it as a graph.\n",
        "writer.add_graph(net, images)\n",
        "writer.flush()"
      ],
      "metadata": {
        "id": "qQRNArDIrxpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing Your Dataset with Embeddings**\n",
        "The 28-by-28 image tiles we’re using can be modeled as 784-dimensional vectors (28 * 28 = 784). It can be instructive to project this to a lower-dimensional representation. The add_embedding() method will project a set of data onto the three dimensions with highest variance, and display them as an interactive 3D chart. The add_embedding() method does this automatically by projecting to the three dimensions with highest variance.\n",
        "\n",
        "Below, we’ll take a sample of our data, and generate such an embedding:"
      ],
      "metadata": {
        "id": "X3Dh3qU9sAXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a random subset of data and corresponding labels\n",
        "def select_n_random(data, labels, n=100):\n",
        "    assert len(data) == len(labels)\n",
        "\n",
        "    perm = torch.randperm(len(data))\n",
        "    return data[perm][:n], labels[perm][:n]\n",
        "\n",
        "# Extract a random subset of data\n",
        "images, labels = select_n_random(training_set.data, training_set.targets)\n",
        "\n",
        "# get the class labels for each image\n",
        "class_labels = [classes[label] for label in labels]\n",
        "\n",
        "# log embeddings\n",
        "features = images.view(-1, 28 * 28)\n",
        "writer.add_embedding(features,\n",
        "                    metadata=class_labels,\n",
        "                    label_img=images.unsqueeze(1))\n",
        "writer.flush()\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "Wb50mn0ssCHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now if you switch to TensorBoard and select the PROJECTOR tab, you should see a 3D representation of the projection. You can rotate and zoom the model. Examine it at large and small scales, and see whether you can spot patterns in the projected data and the clustering of labels.\n",
        "\n",
        "For better visibility, it’s recommended to:\n",
        "\n",
        "Select “label” from the “Color by” drop-down on the left.\n",
        "\n",
        "Toggle the Night Mode icon along the top to place the light-colored images on a dark background."
      ],
      "metadata": {
        "id": "nU6rV5C4sIq8"
      }
    }
  ]
}